{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sketch-2-paint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunalA18/Sketch-2-Paint/blob/dev/Sketch_2_paint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEorC-dPFZvw",
        "outputId": "af54b032-70d6-4e50-b839-b976e867e2ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyv3eO8PkdFP"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur52OQkLKQmP"
      },
      "source": [
        "PATH = 'drive/MyDrive/data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcP7TtSnLkqx"
      },
      "source": [
        "EPOCHS = 20\n",
        "BUFFER_SIZE = 14224\n",
        "BATCH_SIZE = 4\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc3RX7nJR7AV"
      },
      "source": [
        "def load(image_file):\n",
        "    image = tf.io.read_file(image_file)\n",
        "    image = tf.image.decode_png(image)\n",
        "\n",
        "    w = tf.shape(image)[1]\n",
        "\n",
        "    w = w // 2\n",
        "    real_image = image[:, :w, :]\n",
        "    input_image = image[:, w:, :]\n",
        "\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3zO5guk9H5c"
      },
      "source": [
        "PRE PROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZcarQBpTOB3"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "    input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "def random_crop(input_image, real_image):\n",
        "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "    cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    return cropped_image[0], cropped_image[1]\n",
        "\n",
        "def normalize(input_image, real_image):\n",
        "    input_image = (input_image / 127.5) - 1\n",
        "    real_image = (real_image / 127.5) - 1\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
        "    input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "    return input_image, real_image\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgwRoNR9ODD"
      },
      "source": [
        "LOADING DATA SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O1GeXcQLUvtu"
      },
      "source": [
        "def load_image_train(image_file):\n",
        "    input_image, real_image = load(image_file)\n",
        "    input_image, real_image = random_jitter(input_image, real_image)\n",
        "    input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "    return input_image, real_image\n",
        "    \n",
        "train_dataset = tf.data.Dataset.list_files(PATH+'/train/*.png')\n",
        "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O82a7eZj5_rY"
      },
      "source": [
        "def load_image_test(image_file):\n",
        "    input_image, real_image = load(image_file)\n",
        "    input_image, real_image = resize(input_image, real_image,\n",
        "                                   IMG_HEIGHT, IMG_WIDTH)\n",
        "    input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "    return input_image, real_image\n",
        "  \n",
        "test_dataset = tf.data.Dataset.list_files(PATH+'/val/*.png')\n",
        "test_dataset = test_dataset.map(load_image_test)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl4uO3Od9qPn"
      },
      "source": [
        "GENERATOR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_DrI4XnpEWiO"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def downsample(filters, size, shape, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', batch_input_shape=shape, \n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, shape, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2, batch_input_shape=shape,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def buildGenerator():\n",
        "    inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 4, (None, 256, 256, 3), apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4, (None, 128, 128, 64)), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4, (None, 64, 64, 128)), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4, (None, 32, 32, 256)), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4, (None, 16, 16, 512)), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4, (None, 8, 8, 512)), # (bs, 4, 4, 512)\n",
        "        downsample(512, 4, (None, 4, 4, 512)), # (bs, 2, 2, 512)\n",
        "        downsample(512, 4, (None, 2, 2, 512)), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, (None, 1, 1, 512), apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "        upsample(512, 4, (None, 2, 2, 1024), apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "        upsample(512, 4, (None, 4, 4, 1024), apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "        upsample(512, 4, (None, 8, 8, 1024)), # (bs, 16, 16, 1024)\n",
        "        upsample(256, 4, (None, 16, 16, 1024)), # (bs, 32, 32, 512)\n",
        "        upsample(128, 4, (None, 32, 32, 512)), # (bs, 64, 64, 256)\n",
        "        upsample(64, 4, (None, 64, 64, 256)), # (bs, 128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                           strides=2,\n",
        "                                           padding='same',\n",
        "                                           kernel_initializer=initializer,\n",
        "                                           activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "generator = buildGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_to5wX8C_MLa",
        "outputId": "a21b9795-a6ba-43e8-cecf-d367299081a3"
      },
      "source": [
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 128, 128, 64) 3072        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 64, 64, 128)  131584      sequential[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 32, 32, 256)  525312      sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 16, 16, 512)  2099200     sequential_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       (None, 8, 8, 512)    4196352     sequential_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       (None, 4, 4, 512)    4196352     sequential_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_6 (Sequential)       (None, 2, 2, 512)    4196352     sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_7 (Sequential)       (None, 1, 1, 512)    4196352     sequential_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_8 (Sequential)       (None, 2, 2, 512)    4196352     sequential_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 2, 2, 1024)   0           sequential_8[0][0]               \n",
            "                                                                 sequential_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_9 (Sequential)       (None, 4, 4, 512)    8390656     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 4, 1024)   0           sequential_9[0][0]               \n",
            "                                                                 sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_10 (Sequential)      (None, 8, 8, 512)    8390656     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8, 8, 1024)   0           sequential_10[0][0]              \n",
            "                                                                 sequential_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_11 (Sequential)      (None, 16, 16, 512)  8390656     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 16, 16, 1024) 0           sequential_11[0][0]              \n",
            "                                                                 sequential_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_12 (Sequential)      (None, 32, 32, 256)  4195328     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 512)  0           sequential_12[0][0]              \n",
            "                                                                 sequential_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_13 (Sequential)      (None, 64, 64, 128)  1049088     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 64, 64, 256)  0           sequential_13[0][0]              \n",
            "                                                                 sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_14 (Sequential)      (None, 128, 128, 64) 262400      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 128, 128, 128 0           sequential_14[0][0]              \n",
            "                                                                 sequential[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 3)  6147        concatenate_6[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 54,425,859\n",
            "Trainable params: 54,414,979\n",
            "Non-trainable params: 10,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsWmgu9NOCD_"
      },
      "source": [
        "DISCRIMINATOR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uyv7a_ITOEHk"
      },
      "source": [
        "def downs(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', \n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def buildDiscriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "    x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "    down1 = downs(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "    down2 = downs(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "    down3 = downs(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
        "  \n",
        "discriminator = buildDiscriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q1Tfz00uH2iy",
        "outputId": "f938ccfe-5157-4a85-fde7-6efc0bc5ed32"
      },
      "source": [
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "target_image (InputLayer)       [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 256, 256, 6)  0           input_image[0][0]                \n",
            "                                                                 target_image[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_15 (Sequential)      (None, 128, 128, 64) 6144        concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_16 (Sequential)      (None, 64, 64, 128)  131584      sequential_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_17 (Sequential)      (None, 32, 32, 256)  525312      sequential_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 34, 34, 256)  0           sequential_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 31, 31, 512)  2097152     zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 31, 31, 512)  2048        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 31, 31, 512)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 33, 33, 512)  0           leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 30, 30, 1)    8193        zero_padding2d_1[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 2,770,433\n",
            "Trainable params: 2,768,641\n",
            "Non-trainable params: 1,792\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IpgUL8a5IVAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu6bLyOLQIiu"
      },
      "source": [
        "LOSS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VRi5wOo_QJ0f"
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9bzwzfaVpUj"
      },
      "source": [
        "LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hBoAGY4eQK7C"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTbJSLgOVrLS"
      },
      "source": [
        "CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5Twj3YInVlGV"
      },
      "source": [
        "checkpoint_dir = './Sketch2Color_training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cB0eTUKrYM1M"
      },
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "    prediction = model(test_input, training=True)\n",
        "    plt.figure(figsize=(15,15))\n",
        "\n",
        "    display_list = [test_input[0], tar[0], prediction[0]]\n",
        "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EArUDqMQYOYT"
      },
      "source": [
        "import datetime\n",
        "log_dir=\"Sketch2Coloe_logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7SMyzczxZV97"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
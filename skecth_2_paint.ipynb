{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sketch_2_paint.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunalA18/Sketch-2-Paint/blob/dev/Sketch_2_paint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yEorC-dPFZvw",
        "outputId": "f8bc93e1-00f2-4986-fe42-3d5576263326"
      },
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/drive/')\n",
        "'''\n",
        "mounting the data from google drive to colab\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmounting the data from google drive to colab\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyv3eO8PkdFP"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur52OQkLKQmP"
      },
      "source": [
        "PATH = 'drive/MyDrive/data'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcP7TtSnLkqx"
      },
      "source": [
        "EPOCHS = 20\n",
        "BUFFER_SIZE = 14224\n",
        "BATCH_SIZE = 4\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "'''\n",
        "setting the parameters for training the model\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glpr0IcKRm8O"
      },
      "source": [
        "Loading input and output images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc3RX7nJR7AV"
      },
      "source": [
        "def load(image_file):\n",
        "  '''\n",
        "  function load() takes the image path as a parameter and returns an input_image which is the black \n",
        "  and white sketch that we’ll give as an input to the model, and real_image which is the colored image that we want.\n",
        "\n",
        "    Parameters:\n",
        "          image_file(string): a string path\n",
        "    Returns:\n",
        "          input_image:black and white sketch\n",
        "          real_image :colored image which we want\n",
        "  '''\n",
        "    image = tf.io.read_file(image_file)\n",
        "    image = tf.image.decode_png(image)\n",
        "\n",
        "    w = tf.shape(image)[1]\n",
        "\n",
        "    w = w // 2\n",
        "    real_image = image[:, :w, :]\n",
        "    input_image = image[:, w:, :]\n",
        "\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3zO5guk9H5c"
      },
      "source": [
        "PRE PROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZcarQBpTOB3"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  '''\n",
        "  resize() function is used to return the images as 286x286 px.\n",
        "  This is done in order to have a uniform image size if by chance there is a differently sized image in the dataset.\n",
        " \n",
        "\n",
        "    Parameters:\n",
        "          input_image:black and white sketch\n",
        "          real_image :colored image which we want\n",
        "          height: height of the image\n",
        "          width:width of the image\n",
        "    Returns:\n",
        "          input_image:black and white sketch of 286x286 px.\n",
        "          real_image :colored image which we want of 286x286 px.\n",
        "  '''\n",
        "    input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "def random_crop(input_image, real_image):\n",
        "  '''\n",
        "  random_crop() function returns the cropped input and real images which have the desired size of 256x256 px.\n",
        "  And decreasing size from 512x512 px to half of it also helps in speeding up the model training as it is computationally less heavy.\n",
        "    Parameters:\n",
        "         input_image:black and white sketch of 286x286 px.\n",
        "         real_image :colored image which we want of 286x286 px.\n",
        "    Returns:\n",
        "          cropped_image[0]: sketch black and white 256px X 256px image\n",
        "          cropped_image[1]: coloured 256px X 256px image\n",
        "          \n",
        "  '''\n",
        "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "    cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    return cropped_image[0], cropped_image[1]\n",
        "\n",
        "def normalize(input_image, real_image):\n",
        "      '''\n",
        "  normalize() function  normalizes images to [-1, 1].\n",
        "\n",
        "    Parameters:\n",
        "          input_image:black and white sketch after random_jitter for train set and after resize for test set\n",
        "          real_image :colored image which we want after random_jitter for train set and after resize for test set\n",
        "         \n",
        "    Returns:\n",
        "          input_image:black and white sketch after normalisation\n",
        "          real_image :colored image which we want after normalisation\n",
        "  '''\n",
        "    input_image = (input_image / 127.5) - 1\n",
        "    real_image = (real_image / 127.5) - 1\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "@tf.function() #decorative function\n",
        "def random_jitter(input_image, real_image):\n",
        "  '''\n",
        "  In the random_jitter() function  all the previous preprocessing functions are put together \n",
        "  and random images are flipped horizontally. \n",
        "        Parameters:\n",
        "          input_image:black and white sketch\n",
        "          real_image :colored image which we want\n",
        "        Returns:\n",
        "          input_image:black and white sketch after resize and random_crop\n",
        "          real_image :colored image which we want after resize and random_crop \n",
        "  '''\n",
        "    input_image, real_image = resize(input_image, real_image, 286, 286)\n",
        "    input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "    return input_image, real_image\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgwRoNR9ODD"
      },
      "source": [
        "LOADING DATA SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O1GeXcQLUvtu"
      },
      "source": [
        "def load_image_train(image_file):\n",
        "   '''\n",
        "load_image_train() function is used to put together all the previously seen functions and output the final preprocessed image.\n",
        "\n",
        "    Parameters:\n",
        "          image_file(string): a string path input taken\n",
        "    Returns:\n",
        "          input_image:black and white sketch of train dataset\n",
        "          real_image :colored image which we want of train dataset\n",
        "  '''\n",
        "\n",
        "    input_image, real_image = load(image_file)\n",
        "    input_image, real_image = random_jitter(input_image, real_image)\n",
        "    input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "train_dataset = tf.data.Dataset.list_files(PATH+'/train/*.png')\n",
        "# To create a dataset of all files matching a pattern, use tf.data.Dataset.list_files   \n",
        "# For eg:-dataset = tf.data.Dataset.list_files(\"/path/*.txt\") \n",
        "\n",
        "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "# This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, \n",
        "# in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements.\n",
        "#  tf.data.AUTOTUNE ==> which will prompt the tf.data runtime to tune the value dynamically at runtime. \n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "# Randomly shuffles the elements of this dataset.\n",
        "# This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, \n",
        "# replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or \n",
        "# equal to the full size of the dataset is required.\n",
        "\n",
        "# For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, \n",
        "# then shuffle will initially select a random element from only the first 1,000 elements in the buffer. \n",
        "# Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O82a7eZj5_rY"
      },
      "source": [
        "def load_image_test(image_file):\n",
        "     '''\n",
        "  function load() takes the image path as a parameter and returns an input_image which is the black \n",
        "  and white sketch that we’ll give as an input to the model, and real_image which is the colored image that we want.\n",
        "\n",
        "    Parameters:\n",
        "          image_file(string): a string path input taken\n",
        "    Returns:\n",
        "          input_image:black and white sketch of test dataset\n",
        "          real_image :colored image which we want of test dataset\n",
        "  '''\n",
        "\n",
        "    input_image, real_image = load(image_file)\n",
        "    input_image, real_image = resize(input_image, real_image,\n",
        "                                   IMG_HEIGHT, IMG_WIDTH)\n",
        "# resize() function is used to return the images as 286x286 px. This is done in order to have a uniform image size \n",
        "# if by chance there is a differently sized image in the dataset. \n",
        "# And decreasing size from 512x512 px to half of it also helps in speeding up the model training as it is computationally less heavy.\n",
        "    input_image, real_image = normalize(input_image, real_image)\n",
        "# normalize() function, as the name suggests, normalizes images to [-1, 1].\n",
        "    return input_image, real_image\n",
        "  \n",
        "test_dataset = tf.data.Dataset.list_files(PATH+'/val/*.png')\n",
        "test_dataset = test_dataset.map(load_image_test)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "# Explaination of above 3 functions is mentioned in above code block."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl4uO3Od9qPn"
      },
      "source": [
        "GENERATOR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_DrI4XnpEWiO"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def downsample(filters, size, shape, apply_batchnorm=True):\n",
        "   '''\n",
        "  function downsample(): the downsampling stack of layers has Convolutional layers which result in a decrease in the size of the input image.\n",
        "    Parameters:\n",
        "         filters:no of filters used during convolution\n",
        "         size:size of image\n",
        "         shape: shape of image\n",
        "         apply_batchnorm: applying batchnormalisation with a default value true \n",
        "    Returns:\n",
        "         result: result of the downsampled image\n",
        "  '''\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "# Initializer that generates tensors with a normal distribution.Initializers allow you to pre-specify an initialization strategy, \n",
        "# encoded in the Initializer object, without knowing the shape and dtype of the variable being initialized.\n",
        "# Syntax:\n",
        "# tf.random_normal_initializer(\n",
        "#     mean=0.0, stddev=0.05, seed=None\n",
        "# )\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "#  Sequential groups a linear stack of layers into a tf.keras.Model.Inherits From: Model, Layer, Module\n",
        "    result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', batch_input_shape=shape, \n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "# 2D convolution layer (e.g. spatial convolution over images).\n",
        "# This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. \n",
        "# If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.\n",
        "# When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), \n",
        "# e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\"\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "# Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "# Leaky version of a Rectified Linear Unit. Inherits From: Layer, Module.It allows a small gradient when the unit is not active\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, shape, apply_dropout=False):\n",
        "   '''\n",
        "  function upsample(): The decreased image after downsampling goes through the upsampling stack of layers which has kind of “reverse” Convolutional layers, \n",
        "  the size is restored back to 256x256 px. Hence, \n",
        "    Parameters:\n",
        "         filters:no of filters used during convolution\n",
        "         size:size of image\n",
        "         shape: shape of image\n",
        "         apply_dropout: applying dropout with a default value false\n",
        "    Returns:\n",
        "         result: result of the upsampled image\n",
        "  '''\n",
        "\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2, batch_input_shape=shape,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "# Transpose of above conv2D layer\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "# Applies Dropout to the input.Inherits From: Layer, Module\n",
        "# The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. \n",
        "# Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
        "# Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. \n",
        "# When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "# Rectified Linear Unit activation function. Inherits From: Layer, Module. With default values, it returns element-wise max(x, 0)\n",
        "    return result\n",
        "\n",
        "def buildGenerator():\n",
        "   '''\n",
        "  function buildGenerator() gives the output of the Generator Model is a 256x256 px image with 3 output channels.\n",
        "    Returns:\n",
        "          generator model of 256px X 256px\n",
        "  '''\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
        "# Input() is used to instantiate a Keras tensor.\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 4, (None, 256, 256, 3), apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4, (None, 128, 128, 64)), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4, (None, 64, 64, 128)), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4, (None, 32, 32, 256)), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4, (None, 16, 16, 512)), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4, (None, 8, 8, 512)), # (bs, 4, 4, 512)\n",
        "        downsample(512, 4, (None, 4, 4, 512)), # (bs, 2, 2, 512)\n",
        "        downsample(512, 4, (None, 2, 2, 512)), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, (None, 1, 1, 512), apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "        upsample(512, 4, (None, 2, 2, 1024), apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "        upsample(512, 4, (None, 4, 4, 1024), apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "        upsample(512, 4, (None, 8, 8, 1024)), # (bs, 16, 16, 1024)\n",
        "        upsample(256, 4, (None, 16, 16, 1024)), # (bs, 32, 32, 512)\n",
        "        upsample(128, 4, (None, 32, 32, 512)), # (bs, 64, 64, 256)\n",
        "        upsample(64, 4, (None, 64, 64, 256)), # (bs, 128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                           strides=2,\n",
        "                                           padding='same',\n",
        "                                           kernel_initializer=initializer,\n",
        "                                           activation='tanh') # (bs, 256, 256, 3)\n",
        "    # We have used tanh activation function in the last layer instead of RELU as it improves the training of model and produces better result\n",
        "    x = inputs\n",
        "\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "# Functional interface to the Concatenate layer. Returns A tensor, the concatenation of the inputs alongside axis axis.\n",
        "    x = last(x)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "generator = buildGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_to5wX8C_MLa",
        "outputId": "a21b9795-a6ba-43e8-cecf-d367299081a3"
      },
      "source": [
        "generator.summary() #gives the model summary of generator output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 128, 128, 64) 3072        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 64, 64, 128)  131584      sequential[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 32, 32, 256)  525312      sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 16, 16, 512)  2099200     sequential_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       (None, 8, 8, 512)    4196352     sequential_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       (None, 4, 4, 512)    4196352     sequential_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_6 (Sequential)       (None, 2, 2, 512)    4196352     sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_7 (Sequential)       (None, 1, 1, 512)    4196352     sequential_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_8 (Sequential)       (None, 2, 2, 512)    4196352     sequential_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 2, 2, 1024)   0           sequential_8[0][0]               \n",
            "                                                                 sequential_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_9 (Sequential)       (None, 4, 4, 512)    8390656     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 4, 1024)   0           sequential_9[0][0]               \n",
            "                                                                 sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_10 (Sequential)      (None, 8, 8, 512)    8390656     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8, 8, 1024)   0           sequential_10[0][0]              \n",
            "                                                                 sequential_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_11 (Sequential)      (None, 16, 16, 512)  8390656     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 16, 16, 1024) 0           sequential_11[0][0]              \n",
            "                                                                 sequential_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_12 (Sequential)      (None, 32, 32, 256)  4195328     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 512)  0           sequential_12[0][0]              \n",
            "                                                                 sequential_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_13 (Sequential)      (None, 64, 64, 128)  1049088     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 64, 64, 256)  0           sequential_13[0][0]              \n",
            "                                                                 sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_14 (Sequential)      (None, 128, 128, 64) 262400      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 128, 128, 128 0           sequential_14[0][0]              \n",
            "                                                                 sequential[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 3)  6147        concatenate_6[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 54,425,859\n",
            "Trainable params: 54,414,979\n",
            "Non-trainable params: 10,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsWmgu9NOCD_"
      },
      "source": [
        "DISCRIMINATOR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uyv7a_ITOEHk"
      },
      "source": [
        "def downs(filters, size, apply_batchnorm=True):\n",
        "  '''\n",
        "  function downs(): the downsampling stack of layers has Convolutional layers which result in a decrease in the size of the input image.\n",
        "    Parameters:\n",
        "         filters:no of filters used during convolution\n",
        "         size:size of image\n",
        "         shape: shape of image\n",
        "         apply_batchnorm: applying batchnormalisation with a default value true \n",
        "    Returns:\n",
        "         result: result of the image after applying properties of downs function\n",
        "  '''\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02) # Here the parameters are mean and standard deviation\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', \n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def buildDiscriminator():\n",
        "  '''\n",
        "  The primary purpose of the discriminator model is to find out which image is from the actual training dataset and which is an output from the generator model.\n",
        "  '''\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "    x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "    down1 = downs(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "    down2 = downs(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "    down3 = downs(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
        "  \n",
        "discriminator = buildDiscriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q1Tfz00uH2iy",
        "outputId": "f938ccfe-5157-4a85-fde7-6efc0bc5ed32"
      },
      "source": [
        "discriminator.summary() #gives the model summary of the Discriminator  output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "target_image (InputLayer)       [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 256, 256, 6)  0           input_image[0][0]                \n",
            "                                                                 target_image[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_15 (Sequential)      (None, 128, 128, 64) 6144        concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_16 (Sequential)      (None, 64, 64, 128)  131584      sequential_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_17 (Sequential)      (None, 32, 32, 256)  525312      sequential_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 34, 34, 256)  0           sequential_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 31, 31, 512)  2097152     zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 31, 31, 512)  2048        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 31, 31, 512)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 33, 33, 512)  0           leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 30, 30, 1)    8193        zero_padding2d_1[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 2,770,433\n",
            "Trainable params: 2,768,641\n",
            "Non-trainable params: 1,792\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu6bLyOLQIiu"
      },
      "source": [
        "LOSS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VRi5wOo_QJ0f"
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "LAMBDA = 100 # value of LAMBDA is suggested to be kept 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  '''\n",
        "  The loss for the generator is calculated by finding the sigmoid cross-entropy loss of \n",
        "  the output of the generator and an array of ones. This means that we are training it to trick \n",
        "  the discriminator in outputting the value as 1, which means that it is a real image. \n",
        "  Also, for the output to be structurally similar to the target image, we take L1 loss along with it. \n",
        "   Parameters:\n",
        "         disc_generated_output : output generated by discriminator\n",
        "         gen_output :output generated by generator\n",
        "         target : target image which we want\n",
        "         \n",
        "    Returns:\n",
        "         total_gen_loss: formula to calculate the total generator loss is gan_loss + LAMBDA * l1_loss\n",
        "         gan_loss: Minimax GAN loss refers to the minimax simultaneous optimization of the discriminator and generator models.\n",
        "         l1_loss: L1 Loss Function is used to minimize the error which is the sum of the all the \n",
        "                  absolute differences between the true value and the predicted value.\n",
        "  '''\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  '''\n",
        "  For discriminator loss, we take the same sigmoid cross-entropy loss of the real images and an array of ones\n",
        "  and add it with the cross-entropy loss of the output images of the generator model and array of zeros.\n",
        "  real_loss is a sigmoid cross-entropy loss of the real images and an array of ones(since these are the real images).\n",
        "  generated_loss is a sigmoid cross-entropy loss of the generated images and an array of zeros (since these are the fake images).\n",
        " \n",
        "   Parameters:\n",
        "         disc_generated_output : output generated by discriminator for generated images of generator model\n",
        "         disc_real_output :output generated by discriminator for real images\n",
        "     Returns:\n",
        "         total_disc_loss: formula to calculate the total discriminator loss is the sum of real_loss and generated_loss.     \n",
        "  '''\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9bzwzfaVpUj"
      },
      "source": [
        "OPTIMISER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hBoAGY4eQK7C"
      },
      "source": [
        "  '''\n",
        "  Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rates \n",
        "  in order to reduce the losses. Adam Optimizer is one of the best ones to use, in most of the use cases.\n",
        "  '''\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTbJSLgOVrLS"
      },
      "source": [
        "CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5Twj3YInVlGV"
      },
      "source": [
        "#Saving checkpoints at regular intervals is necessary so that you can restore to the latest checkpoint\n",
        "#  and continue from there without losing the previously done hard work by your machines.\n",
        "checkpoint_dir = './Sketch2Color_training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_pR0JerR0dD"
      },
      "source": [
        "Displaying Output Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cB0eTUKrYM1M"
      },
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  '''\n",
        "  The below-given block of code is a basic python function which uses the pyplot module from matplotlib library \n",
        "  to display the predicted images by the generator model.\n",
        "     Parameters:\n",
        "         model : trained model is taken as input \n",
        "         test_input : input from the test set\n",
        "         tar : target image what we want   \n",
        "  '''\n",
        "    prediction = model(test_input, training=True)\n",
        "    plt.figure(figsize=(15,15))\n",
        "\n",
        "    display_list = [test_input[0], tar[0], prediction[0]]\n",
        "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "  # Three integers (nrows, ncols, index). The subplot will take the index position on a grid with nrows rows and ncols columns. \n",
        "  # index starts at 1 in the upper left corner and increases to the right. index can also be a two-tuple \n",
        "  # specifying the (first, last) indices (1-based, and including last) of the subplot,\n",
        "  #  e.g., fig.add_subplot(3, 1, (1, 2)) makes a subplot that spans the upper 2/3 of the figure.\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfw0VjdYR6LK"
      },
      "source": [
        "Logging the Losses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EArUDqMQYOYT"
      },
      "source": [
        "# You can log the important metrics like losses in a file so that you can analyze it as the training progresses on tools like Tensorboard.\n",
        "import datetime\n",
        "log_dir=\"Sketch2Coloe_logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVR6BT-1SBiT"
      },
      "source": [
        "Train Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7SMyzczxZV97"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "  '''\n",
        "  1.  The generator outputs a prediction\n",
        "  2.  The discriminator model is designed to have 2 inputs at a time. For the first time, it is given an input sketch image and the generated image. \n",
        "      The next time it is given the real target image and the generated image.\n",
        "  3.  Now the generator loss and discriminator loss are calculated.\n",
        "  4.  Then, the gradients are calculated from the losses and applied to the optimizers to help the generator produce a better image and also to help discriminator detect the real and generated image with better insights.\n",
        "  5.  All the losses are logged using summary_writer defined previously using tf.summary\n",
        "     Parameters:\n",
        "         input_image : input image taken after all preprocessing steps\n",
        "         target : target image what we want \n",
        "         epoch : no of epochs   \n",
        "  '''\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        " # GradientTape is a mathematical tool for automatic differentiation (autodiff), which is the core functionality of TensorFlow.\n",
        " #  It does not \"track\" the autodiff, it is a key part of performing the autodiff.\n",
        "        gen_output = generator(input_image, training=True)\n",
        "\n",
        "        disc_real_output = discriminator([input_image, target], training=True)\n",
        "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "        tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFwVh-f_STDb"
      },
      "source": [
        "Model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1SUbFYxSRzi"
      },
      "source": [
        "def fit(train_ds, epochs, test_ds):\n",
        "  '''\n",
        "  # Here we iterate over for every epoch and assign the relative time to start variable. \n",
        "  # Then we display an example of the generated image by the generator model. \n",
        "  # This example helps us visualize how the generator gets better at generating better-colored images with every epoch. \n",
        "  # Then we call the train_step function for the model to learn from the calculated losses and gradients. \n",
        "  # And finally, we check if the epoch number is divisible by 5 to save a checkpoint. \n",
        "  # This means that we are saving a checkpoint after every 5 epochs of training are completed.\n",
        "  # After this entire epoch is completed, the start time is subtracted from the final relative time to count the time taken for that particular epoch. \n",
        "      Parameters:\n",
        "         train_ds : train dataset is taken as input\n",
        "         test_ds : test dataset is taken as input\n",
        "         epoch : no of epochs \n",
        "        \n",
        "  '''\n",
        " \n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "# Record the start time\n",
        "        for example_input, example_target in test_ds.take(1):\n",
        "            generate_images(generator, example_input, example_target)\n",
        "        print(\"Epoch: \", epoch)\n",
        "\n",
        "        for n, (input_image, target) in train_ds.enumerate():\n",
        "            print('.', end='')\n",
        "            if (n+1) % 100 == 0:\n",
        "                print()\n",
        "            train_step(input_image, target, epoch)\n",
        "        print()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,time.time()-start))\n",
        "        # end time -start time =time taken for epoch\n",
        "        \n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FevkbJcGSifC"
      },
      "source": [
        "fit(train_dataset, EPOCHS, test_dataset) # To train the data and get the expexted output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwWDhiTGSkpz"
      },
      "source": [
        "Restoring the Latest Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAwHlQzQSjqU"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "# Before moving forward, we must restore the latest checkpoint available in order to load the latest version of the trained model before testing it on the images."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK4HoM7ASvFh"
      },
      "source": [
        "Testing Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNJCSfNhSz0z"
      },
      "source": [
        "for example_input, example_target in test_dataset.take(5):\n",
        "    generate_images(generator, example_input, example_target)\n",
        "# This randomly selects 5 images from the test_dataset and inputs them individually to the Generator Model. \n",
        "# Now the model is trained well enough and predicts near-perfect colored versions of the input sketch images.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rleq9UWAS977"
      },
      "source": [
        "Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c3WCvpjS5cl"
      },
      "source": [
        "generator.save('AnimeColorizationModelv1.h5')\n",
        "# to save the entire model as a .H5 file which is supported by Keras models.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}